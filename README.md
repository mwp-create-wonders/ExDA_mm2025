# ğŸš€ [ExDA: Towards Universal Detection and Plug-and-Play Attribution of AI-Generated Ex-Regulatory Images]
ğŸš€ We will organize the remaining code and update it as soon as possible...

[![Conference](https://img.shields.io/badge/ä¼šè®®ç®€ç§°-å¹´ä»½-blue.svg)](ä½ çš„ä¼šè®®é“¾æ¥)
[![arXiv](https://img.shields.io/badge/arXiv-è®ºæ–‡ID-b31b1b.svg)](ä½ çš„arXivé“¾æ¥)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](LICENSE)
<!-- 
    è¯´æ˜ï¼š
    - ä¸Šé¢çš„å¾½ç« ï¼ˆBadgesï¼‰å¯ä»¥æå‡ä¸“ä¸šæ„Ÿã€‚è¯·å°† "ä¼šè®®ç®€ç§°"ã€"å¹´ä»½"ã€"è®ºæ–‡ID" å’Œå¯¹åº”çš„é“¾æ¥æ›¿æ¢æˆä½ è‡ªå·±çš„ä¿¡æ¯ã€‚
    - å¦‚æœæ²¡æœ‰arXivï¼Œå¯ä»¥åˆ é™¤ç¬¬äºŒè¡Œã€‚
    - Licenseå¯ä»¥æ ¹æ®ä½ çš„é¡¹ç›®é€‰æ‹©ï¼Œä¾‹å¦‚ MIT, Apache 2.0 ç­‰ã€‚
-->

> Our paper has been accepted to **[The 33rd ACM International Conference on Multimedia (MM'25)]**.

<p align="center">
  <img src="framework.jpg" alt="framework" width="700"/>
    
<i> Figure 3: Our Framework Overview: Input images are divided into patches using CLIP. The patches undergo feature extraction
through a frozen ViT and are processed by the SFS-ResNet module, resulting in highly discriminative visual features. Meanwhile, the
patches are processed through a text encoder that incorporates a built-in text feature reduction layer and remains frozen, resulting in
the extraction of consistent semantic features. Finally, they are concatenated and fed into the detection and attribution module. </i>
 </p>

## ğŸ“ Abstract
> As image-generative AI models become increasingly accessible to the public, the demand for content safety has surged. Although model developers have introduced alignment mechanisms to prevent the creation of threatening images, and extensive researches have been conducted on verifying the authenticity of AI-generated images, a significant number of ex-regulatory images have been discovered that fall into regulatory gaps. These images are neither covered by existing alignment mechanisms nor included in the scope of current detection methods. To address this, we introduce ExDA, a detection and attribution framework specifically designed for such ex-regulatory images. ExDA utilizes a frozen CLIP:ViT-L/14 as a visual feature extractor to extract rich and unbiased visual features, complemented by a text feature reduction layer to unify semantic styles. For obtaining highly discriminative features, ExDA introduces an SFS-ResNet network, where each basic layer is replaced with a meticulously designed Multi-Channel Margin Convolution (MMConv). Additionally, a plug-and-play multi-generation model attributor is integrated behind the detector. Given the lack of ex-regulatory images in existing public datasets, we constructed ExImage, a dataset containing 72,000 ex-regulatory images, to validate ExDA's effectiveness. Experiments show that ExDA achieves an average detection accuracy of 99.07\% on ExImage, and demonstrating significant performance improvements of +5.73\% and +10.36\% on GenImage and high-challenge Chameleon datasets respectively in cross-datasets evaluation. Notably, ExDA also achieves excellent performance in attribution tasks, demonstrating its superior ability to identify the intrinsic fingerprints of generative models. Our code is publicly available on the author's homepage.

<br>

## âœ¨ Main Contributions
Our contributions of this paper can be outlined as follows:

*   **(1)**: We develop an efficient framework ExDA for AI-generated ex-regulatory images detection and attribution. ExDA incorporates frozen CLIP:ViT-L/14 as visual feature extraction module to extract unbiased and robust visual features. To effectively process these diverse and complex visual features, a specialized feature processing network called SFS-ResNet is proposed. This network replaces every foundational layer with MMConv, which not only effectively filters out redundant information but also obtains high-frequency and discriminative feature representations.

*   **(2)**: ExDA decouples the frozen text encoder from CLIP:ViT-L/14 and introduces a text feature reduction layer before its multi-head attention. This design aims to minimize the impact of specific image content on the detection process.

*   **(3)**: ExDA also features a plug-and-play and highly extendable attribution plugin, which requires only a small number of unseen generated model images to memorize underlying fingerprint features, resolving accountability issues in real-world scenarios.

*   **(4)**: The ExImage dataset is developed for the detection of ex-regulatory images. Such ex-regulatory images have received minimal research attention, yet they are crucial for social stability and national security.


<br>

## ğŸ› ï¸ Setup

1.  **å…‹éš†æœ¬ä»“åº“**
    ```bash
    git clone https://github.com/[ä½ çš„ç”¨æˆ·å]/[ä½ çš„ä»“åº“å].git
    cd [ä½ çš„ä»“åº“å]
    ```

2.  **åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…ä¾èµ–**
    æˆ‘ä»¬å»ºè®®ä½¿ç”¨ [Anaconda](https://www.anaconda.com/) æ¥ç®¡ç†ç¯å¢ƒï¼š
    ```bash
    conda create -n [ä½ çš„ç¯å¢ƒå] python=3.8
    conda activate [ä½ çš„ç¯å¢ƒå]
    ```
    ç„¶åå®‰è£…æ‰€éœ€çš„åŒ…ï¼š
    ```bash
    pip install -r requirements.txt
    ```
    <!-- è¯´æ˜ï¼šè¯·åœ¨ä½ çš„é¡¹ç›®æ ¹ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ª `requirements.txt` æ–‡ä»¶ï¼Œå¹¶åˆ—å‡ºæ‰€æœ‰ä¾èµ–é¡¹ã€‚ -->

<br>

## â–¶ï¸ è¿è¡Œä»£ç  (Usage)

### 1. æ•°æ®å‡†å¤‡ (Data Preparation)
[è¿™é‡Œè¯´æ˜å¦‚ä½•å‡†å¤‡æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼š]
> è¯·ä» [æ•°æ®é›†é“¾æ¥] ä¸‹è½½æ•°æ®é›†ï¼Œå¹¶å°†å…¶è§£å‹åˆ° `./data/` ç›®å½•ä¸‹ã€‚ç›®å½•ç»“æ„åº”å¦‚ä¸‹æ‰€ç¤ºï¼š
> ```
> .
> â”œâ”€â”€ data
> â”‚   â”œâ”€â”€ dataset_name
> â”‚   â”‚   â”œâ”€â”€ train
> â”‚   â”‚   â”œâ”€â”€ val
> â”‚   â”‚   â””â”€â”€ test
> ...
> ```

### 2. æ¨¡å‹è®­ç»ƒ (Training)
ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥è®­ç»ƒä½ çš„æ¨¡å‹ï¼š
```bash
python train.py --config configs/your_config_file.yaml --output_dir /path/to/save
```
<!-- è¯´æ˜ï¼šè¯·æ ¹æ®ä½ çš„å®é™…è¿è¡Œå‘½ä»¤è¿›è¡Œä¿®æ”¹ã€‚ -->

### 3. æ¨¡å‹è¯„ä¼° (Evaluation)
ä½¿ç”¨æˆ‘ä»¬æä¾›çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼š
```bash
python evaluate.py --checkpoint /path/to/your/checkpoint.pth --config configs/your_config_file.yaml
```
<!-- è¯´æ˜ï¼šå¦‚æœæä¾›é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¯·è¯´æ˜ä¸‹è½½é“¾æ¥å’Œæ”¾ç½®ä½ç½®ã€‚ -->

<br>

## ğŸ† ä¸»è¦ç»“æœ (Results)
[è¿™é‡Œå±•ç¤ºä½ çš„ä¸»è¦å®éªŒç»“æœï¼Œå¯ä»¥ä½¿ç”¨è¡¨æ ¼æˆ–å›¾ç‰‡ã€‚]

**[æ•°æ®é›†A] ä¸Šçš„æ€§èƒ½å¯¹æ¯”**

| æ–¹æ³• (Method)      | æŒ‡æ ‡1 (e.g., Accuracy) | æŒ‡æ ‡2 (e.g., F1-Score) |
| ------------------ | ---------------------- | ---------------------- |
| Baseline           | xx.x%                  | xx.x%                  |
| æ–¹æ³•A (XXX et al.) | xx.x%                  | xx.x%                  |
| **æˆ‘ä»¬çš„æ–¹æ³• (Ours)** | **xx.x%**              | **xx.x%**              |

<br>

## ğŸ“œ å¼•ç”¨ (Citation)
å¦‚æœæˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰æ‰€å¸®åŠ©ï¼Œè¯·è€ƒè™‘å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š
```bibtex
@inproceedings{
  [ä½ çš„å¼•ç”¨æ ‡ç­¾ï¼Œä¾‹å¦‚ï¼šzhang2024yourtitle],
  title={[ä½ çš„è®ºæ–‡æ ‡é¢˜]},
  author={[ä½œè€…ä¸€ and ä½œè€…äºŒ and ...]},
  booktitle={[ä¼šè®®å…¨ç§°]},
  year={[å¹´ä»½]}
}
```

<br>

## ğŸ™ è‡´è°¢ (Acknowledgements)
[æ­¤å¤„å¯ä»¥æ·»åŠ è‡´è°¢ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š]
*   æ„Ÿè°¢ [æŸæŸäºº/æŸæŸç»„ç»‡] æä¾›çš„è®¡ç®—èµ„æºã€‚
*   æœ¬é¡¹ç›®çš„ä»£ç ç»“æ„å‚è€ƒäº† [æŸä¸ªå¼€æºé¡¹ç›®é“¾æ¥]ã€‚
*   æ„Ÿè°¢...

---
